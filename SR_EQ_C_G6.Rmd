---
author: "Group 6"
date: "2025-11-24"
title: "Default of Credit Card Clients in Taiwan Dataset Analysis and Modeling using Logistic Regression in R"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# Setup

## Load Libraries

First, we load the tools (packages) we need to work with the data.

```{r load-libraries}
suppressPackageStartupMessages({
  library(tidyverse)
  library(readxl)
  library(corrplot)
  library(caret)
  library(pROC)
})
```

## Load Data

Let's load our dataset from the Excel file.

```{r load-data, results='hide'}
df <- read_excel("datasets/default of credit card clients.xls", skip = 1)
df |> glimpse()
```

## Data Cleaning

We remove the ID column since it's just a customer identifier and doesn't help predict defaults.

```{r clean-data, results='hide'}
df <- df |> select(-ID)
df |> head()
```

## Summary Statistics

Let's take a quick look at the data to understand what we're working with.

```{r data-summary, results='hide'}
summary(df)
```

## Check Missing Values

We check if there are any missing values that could cause problems later.

```{r check-missing, results='hide'}
colSums(is.na(df))
```

# Exploratory Data Analysis

## Target Variable Distribution

Let's see how many customers defaulted versus didn't default - this tells us about the balance in our data.

```{r target-dist}
df |> ggplot(aes(x = factor(`default payment next month`))) +
  geom_bar(fill = "steelblue", width = 0.5) +
  labs(title = "Distribution of Default Payment",
       x = "Default (0 = No, 1 = Yes)",
       y = "Count") +
  theme_minimal()
```

## Exploring Relationships with Default

Let's look at a few key variables to see how they differ between customers who defaulted and those who didn't. This gives us a feel for which factors matter before we build the model.

```{r explore-default, fig.width=12, fig.height=8}
# Create a few exploratory plots
par(mfrow = c(2, 3))

# 1. Payment Status (PAY_0) vs Default
df_plot <- df %>%
  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default"))) %>%
  group_by(PAY_0, `default payment next month`) %>%
  summarise(Count = n(), .groups = "drop")

plot_pay0 <- ggplot(df_plot, aes(x = factor(PAY_0), y = Count, fill = `default payment next month`)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Payment Status vs Default",
       x = "Payment Status (Month 0)",
       y = "Number of Customers",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() + theme(axis.text.x = element_text(angle = 45))

# 2. Age vs Default (box plot)
plot_age <- ggplot(df %>% mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default"))), 
                   aes(x = `default payment next month`, y = AGE, fill = `default payment next month`)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Age Distribution by Default Status",
       x = "Outcome",
       y = "Age",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(legend.position = "none")

# 3. Credit Limit vs Default
plot_limit <- ggplot(df %>% mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default"))), 
                     aes(x = `default payment next month`, y = LIMIT_BAL, fill = `default payment next month`)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Credit Limit by Default Status",
       x = "Outcome",
       y = "Credit Limit",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(legend.position = "none")

# 4. Payment Amount vs Default
plot_payamt <- ggplot(df %>% mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default"))) %>%
                       filter(PAY_AMT1 < 100000), # Remove extreme outliers for better visualization
                     aes(x = `default payment next month`, y = PAY_AMT1, fill = `default payment next month`)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Payment Amount (PAY_AMT1) by Default Status",
       x = "Outcome",
       y = "Payment Amount (Month 1)",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(legend.position = "none")

# 5. Education vs Default
df_edu <- df %>%
  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default")),
         EDUCATION = factor(EDUCATION, levels = c(0, 1, 2, 3, 4, 5, 6), 
                           labels = c("Unknown", "Graduate", "University", "High School", "Other", "Unknown2", "Unknown3"))) %>%
  group_by(EDUCATION, `default payment next month`) %>%
  summarise(Count = n(), .groups = "drop")

plot_edu <- ggplot(df_edu, aes(x = EDUCATION, y = Count, fill = `default payment next month`)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Education Level vs Default",
       x = "Education Level",
       y = "Number of Customers",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# 6. Marriage Status vs Default
df_marr <- df %>%
  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default")),
         MARRIAGE = factor(MARRIAGE, levels = c(0, 1, 2, 3), 
                          labels = c("Unknown", "Married", "Single", "Divorced"))) %>%
  group_by(MARRIAGE, `default payment next month`) %>%
  summarise(Count = n(), .groups = "drop")

plot_marr <- ggplot(df_marr, aes(x = MARRIAGE, y = Count, fill = `default payment next month`)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Marriage Status vs Default",
       x = "Marriage Status",
       y = "Number of Customers",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Combine plots
library(gridExtra)
grid.arrange(plot_pay0, plot_age, plot_limit, plot_payamt, plot_edu, plot_marr, ncol = 3)
```

### What We Notice

Looking at these plots, some patterns are clear:

-   **Payment Status (PAY_0)**: Customers with recent payment delays show different default rates. The values represent: -1 = paid on time, 1 = one month delay, 2 = two months delay, and so on. People with longer payment delays default much more often.

-   **Age**: Older customers might have different default patterns. The boxes show the spread of ages for each group.

-   **Credit Limit**: Customers with higher credit limits seem to have different default rates. Banks probably gave higher limits to safer customers.

-   **Payment Amounts**: Customers who make bigger payments are less likely to default, which makes sense - if you're paying off your debt, you won't default.

-   **Education & Marriage**: These demographic factors might also play a role, though the patterns are less dramatic.

This exploration helps us see **why** certain features will be important in our model later on.

# Feature Selection and Modeling

## Prepare Data for Modeling

We need to convert the target variable into the right format for our logistic regression model.

```{r prepare-data}
df$`default payment next month` <- factor(df$`default payment next month`, levels = c(0, 1)) # nolint
```

## Full Logistic Regression Model

First, we build a model using all 23 features to see how they work together.

```{r full-model, warning=FALSE, cache=TRUE}
full_model <- glm(`default payment next month` ~ .,
                  data = df,
                  family = binomial(link = "logit"))

summary(full_model)
```

## Individual Feature AIC Evaluation

Next, we test each feature individually to see which ones are good predictors on their own. We use AIC (a measure of model quality) to rank them.

```{r feature-aic-eval, warning=FALSE, cache=TRUE}
feature_names <- colnames(df)[colnames(df) != "default payment next month"]

feature_aic <- data.frame(
  Feature = feature_names,
  AIC = NA
)

for (i in seq_along(feature_names)) {
  formula <- paste("`default payment next month` ~", feature_names[i])
  model <- glm(as.formula(formula), data = df, family = binomial(link = "logit"))
  feature_aic$AIC[i] <- AIC(model)
}

feature_aic <- feature_aic |>
  arrange(AIC) |>
  mutate(Delta_AIC = AIC - min(AIC))

print(feature_aic)
```

From this analysis, we can see that PAY_0 (the most recent payment status) is by far the best predictor, it has the lowest AIC. This tells us right away that how someone paid recently is the strongest indicator of whether they'll default. PAY_2 and PAY_3 (payment status from 2 and 3 months ago) are also good, but not as strong. Bill amounts and age individually don't predict defaults as well.

## Forward Stepwise Feature Selection using AIC

Now we use a smarter approach: instead of using all features or just one, we start with the best predictor (PAY_0) and gradually add other features one by one if they improve the model. This is like building a team where you start with your best player and add teammates that make the team stronger.

```{r forward-selection, warning=FALSE, cache=TRUE}
# Start with the best individual feature (PAY_0)
initial_model <- glm(`default payment next month` ~ PAY_0, data = df, family = binomial(link = "logit"))

# Run forward selection from the initial model with trace to show AIC progression
step_model <- step(initial_model, scope = list(upper = full_model), direction = "forward", trace = 1)

summary(step_model)
```

## Feature Combination and AIC Progression

Here's how the model improved as we added features step by step. Lower AIC means a better model.

```{r feature-combination-table, warning=FALSE}
# Extract selected features from step_model
selected_features <- names(coef(step_model))[-1]  # Exclude intercept

# Create a table showing feature combination progression
feature_progression <- data.frame(
  Step = seq_along(selected_features),
  Feature_Added = selected_features,
  Cumulative_Features = sapply(seq_along(selected_features), function(i) 
    paste(selected_features[1:i], collapse = " + "))
)

# Calculate AIC for each step's model
aics <- numeric(length(selected_features))
aics[1] <- AIC(initial_model)

for(i in 2:length(selected_features)) {
  formula_str <- paste("`default payment next month` ~", 
                       paste(selected_features[1:i], collapse = " + "))
  temp_model <- glm(as.formula(formula_str), data = df, 
                    family = binomial(link = "logit"))
  aics[i] <- AIC(temp_model)
}

feature_progression$AIC <- aics
feature_progression$AIC_Reduction <- c(0, diff(-aics))

print(feature_progression)
```

## AIC Progression Visualization

This chart shows the AIC getting better (going down) as we add more features. See how the improvement slows down at the end, that's exactly when adding more features doesn't help much anymore.

```{r aic-progression-plot, fig.width=10, fig.height=6}
feature_progression_viz <- data.frame(
  Step = seq_along(selected_features),
  Features_Count = seq_along(selected_features),
  AIC = aics
)

ggplot(feature_progression_viz, aes(x = Features_Count, y = AIC)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(color = "steelblue", size = 3) +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.2, color = "gray") +
  labs(title = "AIC Progression Through Forward Feature Selection",
       x = "Number of Features Added",
       y = "AIC Value",
       subtitle = "Demonstrates diminishing returns as features are progressively added") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 11, color = "gray40"),
        axis.text = element_text(size = 10))
```

## Forward Selection Summary

```{r forward-comparison}
cat("Initial Model (PAY_0 only) AIC:", AIC(initial_model), "\n")
cat("Final Selected Model AIC:", AIC(step_model), "\n")
cat("Total AIC Improvement:", AIC(initial_model) - AIC(step_model), "\n")
cat("Total Features Selected:", length(coef(step_model)) - 1, "\n")
```

## Analysis of Feature Selection Results

Using forward selection, we identified 18 out of 23 features as the most important for predicting credit card defaults. This means we dropped 5 less-important features without losing much predictive power, which actually makes our model simpler and easier to explain.

**Where We Started:** When we began with just PAY_0 (the most recent payment status), the model had an AIC of 28,535.57. This shows right away that how someone paid recently is the biggest clue about whether they'll default.

**How Features Were Added:** Looking at our results, we can see that the first few features we added made huge differences: - The first 4 extra features (LIMIT_BAL, PAY_3, PAY_AMT1, BILL_AMT1) each saved about 92-199 AIC points - The next batch (6-12 features) helped less, saving about 10-39 AIC points each - After that, each new feature only helped a tiny bit (0.5-7.8 AIC points), but we kept them because they still improved the model

**Total Improvement:** We went from an AIC of 28,535.57 down to 27,917.81 - that's about 617.76 drop! This shows our 18-feature model fits the data way better than just using one feature, but we won't complicate it with all the 23 features.

**What Types of Features Made the Cut:** We ended up with a good mix: - Payment history from different months (PAY_0 through PAY_5) - Payment amounts and bill amounts - Personal info like marital status, gender, education, and age - Credit limit information

This makes sense because defaults aren't just about one thing, they depend on recent behavior, how much someone owes, and personal circumstances.

## Feature Importance and Coefficients

Now let's look at what each feature actually does in our model. We'll show the coefficient (effect size), confidence intervals (how sure we are), and odds ratios (practical impact).

```{r feature-importance}
# Extract coefficients and confidence intervals
coef_summary <- coef(step_model)
ci_matrix <- confint(step_model, level = 0.95)

model_coef <- data.frame(
  Feature = names(coef_summary),
  Coefficient = coef_summary,
  CI_Lower = ci_matrix[, 1],
  CI_Upper = ci_matrix[, 2]
) |>
  mutate(Coefficient = round(Coefficient, 6),
         CI_Lower = round(CI_Lower, 6),
         CI_Upper = round(CI_Upper, 6))

p_values <- summary(step_model)$coefficients[, "Pr(>|z|)"]
model_coef$P_Value <- p_values[match(model_coef$Feature, names(p_values))]
model_coef$Significance <- ifelse(model_coef$P_Value < 0.001, "***",
                                   ifelse(model_coef$P_Value < 0.01, "**",
                                          ifelse(model_coef$P_Value < 0.05, "*", "")))

# Add odds ratios and their confidence intervals
odds_ratios <- exp(coef_summary)
odds_ci <- exp(ci_matrix)

model_coef$Odds_Ratio <- round(odds_ratios[match(model_coef$Feature, names(odds_ratios))], 4)
model_coef$OR_CI_Lower <- round(odds_ci[match(model_coef$Feature, rownames(odds_ci)), 1], 4)
model_coef$OR_CI_Upper <- round(odds_ci[match(model_coef$Feature, rownames(odds_ci)), 2], 4)

print(model_coef)
```

## What Our Coefficients Tell Us

Looking at our model, one thing stands out clearly: **payment history is everything**. How someone has been paying, especially recently, is by far the strongest predictor of whether they'll default:

**The Biggest Risk Factors (Make Default More Likely):**

-   **PAY_0 (coefficient: 0.577)** - This is the powerhouse. PAY_0 measures recent payment status, and it's the single strongest predictor. The confidence interval is [0.536, 0.619], which is tight and doesn't include zero, meaning we're really confident this feature matters alot. Basically, if someone has had recent payment delays, their default risk jump up.

-   **PAY_2 and PAY_3 (coefficients: 0.107 and 0.068)** - These measure payment status from 2 and 3 months ago. Interestingly, they're still important even though they're older data, which suggests that if someone has a pattern of paying late, it keeps affecting their risk.

-   **PAY_5 (positive but smaller coefficient)** - Payment status from 5 months ago still matters, but the effect gets weaker as we go back in time.

**The Protective Factors (Make Default Less Likely):**

-   **MARRIAGE (coefficient: -0.156, CI: [-0.230, -0.082])** - Married customers have a lower default risk. [1 = married, 2 = single, 3 = others] The negative coefficient shows that marital stability is a protective factor against default.

-   **EDUCATION (coefficient: -0.105, CI: [-0.154, -0.056])** - Customers with higher education levels are less likely to default. [1 = graduate school, 2 = university, 3 = high school, 4 = others] This could reflect higher income stability or better financial management.

-   **SEX (coefficient: -0.104, CI: [-0.176, -0.032])** - One gender has lower default risk than the other. [1 = male, 2 = female] The negative coefficient indicates this protective effect is statistically significant.

-   **Payment Amounts (PAY_AMT1-6)** - Customers who make larger payments are much less likely to default. This actually makes an intuitive sense, because if you're paying off your card, you're not going to default.

-   **LIMIT_BAL (very small negative coefficient)** - Higher credit limits are associated with lower default risk, probably because banks give bigger limits to safer customers.

**Statistical Confidence:** Most coefficients show very strong evidence (p < 0.001, marked with ***), meaning we're really confident these effects are real. A couple of features like PAY_AMT3 and BILL_AMT2 aren't quite statistically significant on their own, but the model still includes them because they help reduce AIC overall, which improves our model's fit.

### Odds Ratios Impact

While coefficients tell us direction and statistical significance, **odds ratios** (shown in the table above) tell us the practical impact in a more intuitive way. 

-   **Odds Ratio = 1.00**: No effect on default risk
-   **Odds Ratio \> 1.00**: Increases default risk (every unit increase multiplies odds by this amount)
-   **Odds Ratio \< 1.00**: Decreases default risk (protective factor)

**From Our Model:**

-   **PAY_0 (OR = 1.78, 95% CI: [1.71, 1.86])**: For each one-unit increase in recent payment delay status, the odds of default multiply by about 1.78, which is about **78% increase** in default odds. This is huge and it's our strongest predictor. The narrow confidence interval shows we're very confident this effect is real.

-   **MARRIAGE (OR = 0.86, 95% CI: [0.79, 0.92])**: Being married multiplies the odds of default by 0.86, which means it **reduces** the odds by about 14%. Married people are actually lower risk.

-   **EDUCATION (OR = 0.90, 95% CI: [0.86, 0.95])**: Higher education level multiplies odds by 0.90 - a **10% reduction** in default odds. Education is a meaningful protective factor.

-   **PAY_AMT1 (OR \~= 0.9999)**: The odds ratio is extremely close to 1 because payment amounts are in thousands. Each additional dollar has a tiny protective effect, but the aggregate effect of large payments is meaningful.

Odds ratios make it much easier to communicate results: an odds ratio of 1.78 is "a big deal" - every step up in payment delay increases your default risk by 78%. An odds ratio of 0.90 is "meaningful but modest" - a real protective effect but not dramatic.

## Feature Importance Visualization with Confidence Intervals

This chart shows each feature's effect, with bars showing the coefficient size and error bars showing our 95% confidence intervals. Red means it increases default risk, green means it reduces it.

```{r coefficient-plot, fig.width=10, fig.height=8}
# Prepare data for plotting (exclude intercept)
coef_plot_data <- model_coef |>
  filter(Feature != "(Intercept)") |>
  arrange(Coefficient) |>
  mutate(Feature = factor(Feature, levels = Feature),
         Effect_Type = ifelse(Coefficient > 0, "Risk Factor", "Protective Factor"))

ggplot(coef_plot_data, aes(x = Coefficient, y = Feature, fill = Effect_Type)) +
  geom_col(width = 0.6, alpha = 0.8) +
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.3, color = "black", linewidth = 0.8) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black", linewidth = 1) +
  scale_fill_manual(values = c("Risk Factor" = "#e74c3c", "Protective Factor" = "#27ae60")) +
  labs(title = "Model Coefficients with 95% Confidence Intervals",
       x = "Coefficient Value",
       y = "Feature",
       fill = "Effect Type",
       subtitle = "Red: Increases default risk | Green: Decreases default risk | Error bars show 95% CI") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 11, color = "gray40"),
        axis.text.y = element_text(size = 9),
        axis.text.x = element_text(size = 10),
        legend.position = "bottom")
```

# Model Evaluation

## Train-Test Split with Stratified Sampling

Our data is imbalanced - most customers don't default (78%), and only about 22% do. If we randomly split the data, we might accidentally get a test set with way more or fewer defaults than the real data. So instead, we use "stratified" sampling: we make sure our training set and test set have about the same percentage of defaults as the overall dataset. This gives us a fair test.

```{r train-test-split, results='hide'}
set.seed(123)
# Use stratified sampling to maintain class proportions
train_idx <- createDataPartition(df$`default payment next month`, p = 0.7, list = FALSE)
train_data <- df[train_idx, ]
test_data <- df[-train_idx, ]

cat("Dataset Class Distribution:\n")
cat("Overall - Default: ", round(mean(df$`default payment next month` == 1) * 100, 2), "%\n", sep="")
cat("Training Set - Default: ", round(mean(train_data$`default payment next month` == 1) * 100, 2), "%\n", sep="")
cat("Test Set - Default: ", round(mean(test_data$`default payment next month` == 1) * 100, 2), "%\n\n", sep="")

cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")
```

## Train Model on Training Set

Now we train our final model using the 18 features we selected, but only on the training data (not peeking at the test set).

```{r train-model, warning=FALSE, results='hide'}
train_model <- glm(`default payment next month` ~ PAY_0 + PAY_2 + PAY_3 + PAY_5 + 
                     LIMIT_BAL + PAY_AMT1 + MARRIAGE + SEX + EDUCATION + 
                     AGE + PAY_AMT2 + PAY_AMT4 + PAY_AMT3 + PAY_AMT5 + BILL_AMT1 + 
                     PAY_AMT6 + BILL_AMT2 + BILL_AMT3,
                   data = train_data,
                   family = binomial(link = "logit"))

summary(train_model)
```

## Model Predictions

Now we use the trained model to make predictions on the test set we put aside earlier.

```{r predictions, results='hide'}
pred_prob <- predict(train_model, newdata = test_data, type = 'response')
pred_class <- ifelse(pred_prob > 0.5, 1, 0)

cat("Prediction probability range:", min(pred_prob), "to", max(pred_prob), "\n")
cat("Number of predicted defaults:", sum(pred_class == 1), "\n")
cat("Number of predicted non-defaults:", sum(pred_class == 0), "\n")
```

## Confusion Matrix and Performance Metrics

Let's see how our predictions compare to reality using a confusion matrix.

```{r confusion-matrix, results='hide'}
actual <- as.factor(as.numeric(test_data$`default payment next month`) - 1)
pred_class <- as.factor(pred_class)

cm <- confusionMatrix(pred_class, actual, positive = "1")
print(cm)
```

## Confusion Matrix Visualization

Here's the confusion matrix shown as a color chart. The darker the red, the more cases in that cell. You want to see the diagonal (top-left and bottom-right) bright red, because those are the correct predictions.

```{r confusion-matrix-heatmap, fig.width=8, fig.height=6}
# Extract confusion matrix values
cm_matrix <- cm$table

# Properly convert matrix to long format preserving the 2D structure
cm_long <- expand_grid(
  Actual = as.numeric(colnames(cm_matrix)),
  Prediction = as.numeric(rownames(cm_matrix))
) %>%
  mutate(
    Count = c(cm_matrix),
    Actual_Label = factor(Actual, labels = c("No Default (0)", "Default (1)")),
    Prediction_Label = factor(Prediction, labels = c("No Default (0)", "Default (1)"))
  )

# Create heatmap with correct orientation
ggplot(cm_long, aes(x = Actual_Label, y = Prediction_Label, fill = Count)) +
  geom_tile(color = "black", linewidth = 1) +
  geom_text(aes(label = Count), fontface = "bold", size = 6, color = "white") +
  scale_fill_gradient(low = "#fee5d9", high = "#a50f15") +
  scale_y_discrete(limits = rev(levels(cm_long$Prediction_Label))) +
  labs(title = "Confusion Matrix Heatmap",
       x = "Actual Class",
       y = "Predicted Class",
       fill = "Count",
       subtitle = "Correct predictions on diagonal (TN top-left=6815, TP bottom-right=492)") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 10, color = "gray40"),
        axis.text = element_text(size = 11),
        panel.grid = element_blank())
```

## Extract and Display Performance Metrics

Let's calculate all the important numbers that tell us how well our model did.

```{r extract-metrics}
metrics <- cm$byClass
overall_metrics <- cm$overall

accuracy <- overall_metrics["Accuracy"]
sensitivity <- metrics["Sensitivity"]
specificity <- metrics["Specificity"]
precision <- metrics["Pos Pred Value"]
balanced_acc <- metrics["Balanced Accuracy"]

# Calculate AUC for display
roc_obj <- roc(actual, pred_prob)
auc_value <- auc(roc_obj)
```

## Our Results

```{r display-metrics}
cat("Performance Metrics with 95% Confidence Intervals:\n\n")
cat("Accuracy:              ", round(overall_metrics["Accuracy"], 4), "\n")
cat("95% CI:                [", round(overall_metrics["AccuracyLower"], 4), ", ", 
    round(overall_metrics["AccuracyUpper"], 4), "]\n\n", sep="")
cat("Sensitivity (Recall):  ", round(metrics["Sensitivity"], 4), "\n")
cat("Specificity:           ", round(metrics["Specificity"], 4), "\n")
cat("Precision:             ", round(metrics["Pos Pred Value"], 4), "\n")
cat("Balanced Accuracy:     ", round(metrics["Balanced Accuracy"], 4), "\n")
cat("AUC:                   ", round(auc_value, 4), "\n")
```

## What These Metrics Mean

-   **Accuracy**: How many predictions are correct overall (out of all predictions)
-   **Sensitivity**: Of people who actually defaulted, how many did we catch?
-   **Specificity**: Of people who didn't default, how many did we correctly identify?
-   **Precision**: When we predict someone will default, how often are we right?
-   **Balanced Accuracy**: Average of sensitivity and specificity - a fairer measure when data is imbalanced

## How Well Did Our Model Work?

### Performance Metrics Summary

| Metric            | Value  | Interpretation                                         |
|-------------------|--------|--------------------------------------------------------|
| Accuracy          | 81.20% | Overall correct predictions across both classes        |
| Sensitivity       | 24.72% | Proportion of actual defaults correctly identified     |
| Specificity       | 97.23% | Proportion of actual non-defaults correctly identified |
| Precision         | 71.72% | Of predicted defaults, how many are actually correct   |
| Balanced Accuracy | 60.98% | Average performance accounting for class imbalance     |
| AUC               | 0.7250 | Discrimination ability between default and non-default |

### Detailed Performance Analysis

**Accuracy (81.20% with 95% CI: [80.38%, 82.00%])** - Our model correctly predicts whether a customer will default or not 81.20% of the time. This means that out of every 100 customers in the test set, the model makes correct predictions for about 81 of them. The narrow confidence interval shows that this result is fairly reliable and wouldn't change much if we tested on different data.

**Sensitivity (24.72%): Default Detection Rate** - This is actually one of the challenging aspects of our model. The model only catches about 24.72% of customers who actually default. In practical terms, if there are 100 customers who will truly default, our model would identify only about 25 of them. This means we miss 75% of the actual defaults (false negatives). This low sensitivity happens because our model is being conservative - it's trying hard not to falsely alarm customers who won't default.

**Specificity (97.23%): Non-Default Identification** - On the flip side, our model is very good at identifying customers who will NOT default. It correctly identifies 97.23% of non-defaulters. This is actually a strength because it means we rarely bother customers who are good payers with unnecessary interventions or warnings.

**Precision (71.72%): Prediction Reliability** - When our model predicts that someone will default, it's correct about 71.72% of the time. This means if we act on the model's default predictions, we can be fairly confident that most of those customers are actually at risk.

**Balanced Accuracy (60.98%)** - Since our dataset is imbalanced (22% defaults, 78% non-defaults), we use balanced accuracy to get a fair picture. This gives equal weight to sensitivity and specificity, showing that on average, our model performs moderately well on both classes.

**AUC (0.7250): Model Discrimination Ability** - This tells us how well our model can distinguish between defaulters and non-defaulters at different probability thresholds. A score of 0.72 is reasonably good (better than 0.50 which is random), though there's definitely room for improvement toward 0.80 or higher.

### What the Trade-offs Mean

Notice something interesting: our model is very cautious. It's fantastic at identifying customers who are fine (97% specificity) but misses quite a few who actually default (only 25% sensitivity). Why? Because:

1.  Most people don't default, so the model learns that "non-default" is the safer prediction
2.  The 0.5 cutoff (we say "default" if probability > 0.5) was kind of arbitrary - we could change it
3.  Our training data naturally has this imbalance, so the model reflects it

In the real world, this kind of model would probably be used to flag high-risk customers for extra attention, not as the only tool for making decisions about who gets credit.

### Practical Uses and Limitations

-   **Good at**: Not bothering customers who are fine (97% of non-defaulters get flagged as safe)

-   **Not great at**: Catching all the risky customers (we'd miss 75% of actual defaults)

-   **Works best for**: Obvious risk cases like recent late payments or very old accounts

-   **Might miss**: Defaults by customers who seemed fine until suddenly they weren't

-   **Could be improved**: If we adjust the threshold to be more aggressive, we'd catch more defaults, but we'd also bother more good customers

## ROC Curve and AUC

The ROC curve shows us how good our model is at distinguishing between defaulters and non-defaulters, across all possible decision thresholds.

```{r roc-curve, fig.width=8, fig.height=6}
# Plot ROC curve (auc_value was already calculated in extract-metrics chunk)
plot(roc_obj, main = "ROC Curve", col = "steelblue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright", paste("AUC =", round(auc_value, 4)))
```

### Understanding Our AUC Score

**AUC: 0.7250**

AUC tells us how well the model can separate defaulters from non-defaulters..

The ROC curve itself shows the different trade-offs we face. Right now we're using 0.5 as the cutoff (predict default if probability > 0.5). But we could slide that threshold left or right depending on what matters more to us: catching defaults or avoiding false alarms. The curve shows all those options.

## Conclusion

Building a predictive model for credit card defaults is not a simple task, and our model reflects this complexity. The results show that we have developed a reasonably effective tool, though it comes with both strengths and limitations that must be understood before implementation.

On the positive side, our model achieves 81% overall accuracy, correctly classifying most customers in our test set. When the model predicts that a customer is safe, it is correct approximately 97% of the time. This high specificity means that the model rarely triggers false alarms for good customers, which is valuable for maintaining customer relationships. Additionally, when the model does flag someone as being at risk of default, it is correct about 72% of the time, making these predictions meaningful enough to warrant further investigation. Perhaps most importantly, the risk factors our model identified align well with intuition: recent payment delays indicate higher risk, while factors like marital stability and higher education levels are protective.

However, there are significant limitations we must acknowledge. The model only catches about 25% of customers who actually default, which means we miss three-quarters of the people who will truly default. This happens because most customers don't default, so the model learns to be very conservative in its predictions, favoring the safer assumption. The AUC of 0.72 indicates decent discriminatory ability but leaves room for improvement. These limitations suggest that relying solely on this model for credit decisions would be insufficient.

The practical application of this model is therefore best understood as part of a larger risk management system. Rather than using it as the sole decision-making tool, the model works best as a screening mechanism to identify the highest-confidence default cases for closer manual review. Banks might use these predictions to flag customers for additional scrutiny, to combine with other assessment methods, or to adjust their lending strategies accordingly. The trade-off is clear: if we wanted to catch more defaults, we could lower the decision threshold, but this would inevitably increase false alarms for good customers.

In all, our analysis confirms what many financial institutions already understand: payment history is the strongest predictor of default risk. Recent payment behavior, more than any other factor, reveals a customer's likelihood to default. Our model provides empirical support for this intuition and demonstrates that while predicting defaults is challenging, it is possible to build models that add value to decision-making processes when used appropriately within a broader framework.
