---
author: "Group 6"
date: "2025-11-24"
title: "Default of Credit Card Clients in Taiwan Dataset Analysis and Modeling using Logistic Regression in R"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = FALSE, message = FALSE)
```

# Setup

## Load Libraries

First, we load the tools (packages) we need to work with the data.

```{r load-libraries}
library(tidyverse)
library(readxl)
library(corrplot)
library(caret)
library(pROC)
```

## Load Data

Let's load our dataset from the Excel file.

```{r load-data, results='hide'}
df <- read_excel("datasets/default of credit card clients.xls", skip = 1)
df |> glimpse()
```

## Data Cleaning

We remove the ID column since it's just a customer identifier and doesn't help predict defaults.

```{r clean-data, results='hide'}
df <- df |> select(-ID)
df |> head()
```

## Summary Statistics

Let's take a quick look at the data to understand what we're working with.

```{r data-summary, results='hide'}
summary(df)
```

## Check Missing Values

We check if there are any missing values that could cause problems later.

```{r check-missing, results='hide'}
colSums(is.na(df))
```

# Exploratory Data Analysis

## Target Variable Distribution

Let's see how many customers defaulted versus didn't default - this tells us about the balance in our data.

```{r target-dist}
df |> ggplot(aes(x = factor(`default payment next month`))) +
  geom_bar(fill = "steelblue", width = 0.5) +
  labs(title = "Distribution of Default Payment",
       x = "Default (0 = No, 1 = Yes)",
       y = "Count") +
  theme_minimal()
```

## Exploring Relationships with Default

Let's look at a few key variables to see how they differ between customers who defaulted and those who didn't. This gives us a feel for which factors matter before we build the model.

```{r explore-default, fig.width=12, fig.height=8}
# Create a few exploratory plots
par(mfrow = c(2, 3))

# 1. Payment Status (PAY_0) vs Default
df_plot <- df |> 
  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default"))) |> 
  group_by(PAY_0, `default payment next month`) |> 
  summarise(Count = n(), .groups = "drop")

plot_pay0 <- ggplot(df_plot, aes(x = factor(PAY_0), y = Count, fill = `default payment next month`)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Payment Status vs Default",
       x = "Payment Status (Month 0)",
       y = "Number of Customers",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() + theme(axis.text.x = element_text(angle = 45))

# 2. Age vs Default (box plot)
plot_age <- ggplot(df |>  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default"))), 
                   aes(x = `default payment next month`, y = AGE, fill = `default payment next month`)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Age Distribution by Default Status",
       x = "Outcome",
       y = "Age",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(legend.position = "none")

# 3. Credit Limit vs Default
plot_limit <- ggplot(df |>  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default"))), 
                     aes(x = `default payment next month`, y = LIMIT_BAL, fill = `default payment next month`)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Credit Limit by Default Status",
       x = "Outcome",
       y = "Credit Limit",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(legend.position = "none")

# 4. Payment Amount vs Default
plot_payamt <- ggplot(df |>  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default"))) |> 
                       filter(PAY_AMT1 < 100000), # Remove extreme outliers for better visualization
                     aes(x = `default payment next month`, y = PAY_AMT1, fill = `default payment next month`)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "Payment Amount (PAY_AMT1) by Default Status",
       x = "Outcome",
       y = "Payment Amount (Month 1)",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(legend.position = "none")

# 5. Education vs Default
df_edu <- df |> 
  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default")),
         EDUCATION = factor(EDUCATION, levels = c(0, 1, 2, 3, 4, 5, 6), 
                           labels = c("Unknown", "Graduate", "University", "High School", "Other", "Unknown2", "Unknown3"))) |> 
  group_by(EDUCATION, `default payment next month`) |> 
  summarise(Count = n(), .groups = "drop")

plot_edu <- ggplot(df_edu, aes(x = EDUCATION, y = Count, fill = `default payment next month`)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Education Level vs Default",
       x = "Education Level",
       y = "Number of Customers",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# 6. Marriage Status vs Default
df_marr <- df |> 
  mutate(`default payment next month` = factor(`default payment next month`, labels = c("No Default", "Default")),
         MARRIAGE = factor(MARRIAGE, levels = c(0, 1, 2, 3), 
                          labels = c("Unknown", "Married", "Single", "Divorced"))) |> 
  group_by(MARRIAGE, `default payment next month`) |> 
  summarise(Count = n(), .groups = "drop")

plot_marr <- ggplot(df_marr, aes(x = MARRIAGE, y = Count, fill = `default payment next month`)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  labs(title = "Marriage Status vs Default",
       x = "Marriage Status",
       y = "Number of Customers",
       fill = "Outcome") +
  scale_fill_manual(values = c("No Default" = "#27ae60", "Default" = "#e74c3c")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Combine plots
library(gridExtra)
grid.arrange(plot_pay0, plot_age, plot_limit, plot_payamt, plot_edu, plot_marr, ncol = 3)
```


# Feature Selection and Modeling

## Prepare Data for Modeling

We need to convert the target variable into the right format for our logistic regression model.

```{r prepare-data}
df$`default payment next month` <- factor(df$`default payment next month`, levels = c(0, 1)) # nolint
```

## Full Logistic Regression Model

First, we build a model using all 23 features to see how they work together.

```{r full-model, warning=FALSE, cache=TRUE}
full_model <- glm(`default payment next month` ~ .,
                  data = df,
                  family = binomial(link = "logit"))

summary(full_model)
```

## Individual Feature AIC Evaluation

Next, we test each feature individually to see which ones are good predictors on their own. We use AIC (a measure of model quality) to rank them.

```{r feature-aic-eval, warning=FALSE, cache=TRUE}
feature_names <- colnames(df)[colnames(df) != "default payment next month"]

feature_aic <- data.frame(
  Feature = feature_names,
  AIC = NA
)

for (i in seq_along(feature_names)) {
  formula <- paste("`default payment next month` ~", feature_names[i])
  model <- glm(as.formula(formula), data = df, family = binomial(link = "logit"))
  feature_aic$AIC[i] <- AIC(model)
}

feature_aic <- feature_aic |>
  arrange(AIC) |>
  mutate(Delta_AIC = AIC - min(AIC))

print(feature_aic)
```

From this analysis, we can see that PAY_0 (the most recent payment status) is by far the best predictor, it has the lowest AIC. This tells us right away that how someone paid recently is the strongest indicator of whether they'll default. PAY_2 and PAY_3 (payment status from 2 and 3 months ago) are also good, but not as strong. Bill amounts and age individually don't predict defaults as well.

## Forward Stepwise Feature Selection using AIC

Now we use a smarter approach: instead of using all features or just one, we start with the best predictor (PAY_0) and gradually add other features one by one if they improve the model. This is like building a team where you start with your best player and add teammates that make the team stronger.

```{r forward-selection, warning=FALSE, cache=TRUE}
# Start with the best individual feature (PAY_0)
initial_model <- glm(`default payment next month` ~ PAY_0, data = df, family = binomial(link = "logit"))

# Run forward selection from the initial model with trace to show AIC progression
step_model <- step(initial_model, scope = list(upper = full_model), direction = "forward", trace = 1)

summary(step_model)
```

## Feature Combination and AIC Progression

Here's how the model improved as we added features step by step. Lower AIC means a better model.

```{r feature-combination-table, warning=FALSE}
# Extract selected features from step_model
selected_features <- names(coef(step_model))[-1]  # Exclude intercept

# Create a table showing feature combination progression
feature_progression <- data.frame(
  Step = seq_along(selected_features),
  Feature_Added = selected_features,
  Cumulative_Features = sapply(seq_along(selected_features), function(i) 
    paste(selected_features[1:i], collapse = " + "))
)

# Calculate AIC for each step's model
aics <- numeric(length(selected_features))
aics[1] <- AIC(initial_model)

for(i in 2:length(selected_features)) {
  formula_str <- paste("`default payment next month` ~", 
                       paste(selected_features[1:i], collapse = " + "))
  temp_model <- glm(as.formula(formula_str), data = df, 
                    family = binomial(link = "logit"))
  aics[i] <- AIC(temp_model)
}

feature_progression$AIC <- aics
feature_progression$AIC_Reduction <- c(0, diff(-aics))

print(feature_progression)
```

## AIC Progression Visualization

This chart shows the AIC getting better (going down) as we add more features. See how the improvement slows down at the end, that's exactly when adding more features doesn't help much anymore.

```{r aic-progression-plot, fig.width=10, fig.height=6}
feature_progression_viz <- data.frame(
  Step = seq_along(selected_features),
  Features_Count = seq_along(selected_features),
  AIC = aics
)

ggplot(feature_progression_viz, aes(x = Features_Count, y = AIC)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(color = "steelblue", size = 3) +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.2, color = "gray") +
  labs(title = "AIC Progression Through Forward Feature Selection",
       x = "Number of Features Added",
       y = "AIC Value",
       subtitle = "Demonstrates diminishing returns as features are progressively added") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 11, color = "gray40"),
        axis.text = element_text(size = 10))
```

## Forward Selection Summary

```{r forward-comparison}
cat("Initial Model (PAY_0 only) AIC:", AIC(initial_model), "\n")
cat("Final Selected Model AIC:", AIC(step_model), "\n")
cat("Total AIC Improvement:", AIC(initial_model) - AIC(step_model), "\n")
cat("Total Features Selected:", length(coef(step_model)) - 1, "\n")
```


## Feature Importance and Coefficients

Now let's look at what each feature actually does in our model. We'll show the coefficient (effect size), confidence intervals (how sure we are), and odds ratios (practical impact).

```{r feature-importance}
# Extract coefficients and confidence intervals
coef_summary <- coef(step_model)
ci_matrix <- confint(step_model, level = 0.95)

model_coef <- data.frame(
  Feature = names(coef_summary),
  Coefficient = coef_summary,
  CI_Lower = ci_matrix[, 1],
  CI_Upper = ci_matrix[, 2]
) |>
  mutate(Coefficient = round(Coefficient, 6),
         CI_Lower = round(CI_Lower, 6),
         CI_Upper = round(CI_Upper, 6))

p_values <- summary(step_model)$coefficients[, "Pr(>|z|)"]
model_coef$P_Value <- p_values[match(model_coef$Feature, names(p_values))]
model_coef$Significance <- ifelse(model_coef$P_Value < 0.001, "***",
                                   ifelse(model_coef$P_Value < 0.01, "**",
                                          ifelse(model_coef$P_Value < 0.05, "*", "")))

# Add odds ratios and their confidence intervals
odds_ratios <- exp(coef_summary)
odds_ci <- exp(ci_matrix)

model_coef$Odds_Ratio <- round(odds_ratios[match(model_coef$Feature, names(odds_ratios))], 4)
model_coef$OR_CI_Lower <- round(odds_ci[match(model_coef$Feature, rownames(odds_ci)), 1], 4)
model_coef$OR_CI_Upper <- round(odds_ci[match(model_coef$Feature, rownames(odds_ci)), 2], 4)

print(model_coef)
```


## Feature Importance Visualization with Confidence Intervals

This chart shows each feature's effect, with bars showing the coefficient size and error bars showing our 95% confidence intervals. Red means it increases default risk, green means it reduces it.

```{r coefficient-plot, fig.width=10, fig.height=8}
# Prepare data for plotting (exclude intercept)
coef_plot_data <- model_coef |>
  filter(Feature != "(Intercept)") |>
  arrange(Coefficient) |>
  mutate(Feature = factor(Feature, levels = Feature),
         Effect_Type = ifelse(Coefficient > 0, "Risk Factor", "Protective Factor"))

ggplot(coef_plot_data, aes(x = Coefficient, y = Feature, fill = Effect_Type)) +
  geom_col(width = 0.6, alpha = 0.8) +
  geom_errorbarh(aes(xmin = CI_Lower, xmax = CI_Upper), height = 0.3, color = "black", linewidth = 0.8) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black", linewidth = 1) +
  scale_fill_manual(values = c("Risk Factor" = "#e74c3c", "Protective Factor" = "#27ae60")) +
  labs(title = "Model Coefficients with 95% Confidence Intervals",
       x = "Coefficient Value",
       y = "Feature",
       fill = "Effect Type",
       subtitle = "Red: Increases default risk | Green: Decreases default risk | Error bars show 95% CI") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 11, color = "gray40"),
        axis.text.y = element_text(size = 9),
        axis.text.x = element_text(size = 10),
        legend.position = "bottom")
```

# Model Evaluation

## Train-Test Split with Stratified Sampling

Our data is imbalanced - most customers don't default (78%), and only about 22% do. If we randomly split the data, we might accidentally get a test set with way more or fewer defaults than the real data. So instead, we use "stratified" sampling: we make sure our training set and test set have about the same percentage of defaults as the overall dataset. This gives us a fair test.

```{r train-test-split, results='hide'}
set.seed(123)
# Use stratified sampling to maintain class proportions
train_idx <- createDataPartition(df$`default payment next month`, p = 0.7, list = FALSE)
train_data <- df[train_idx, ]
test_data <- df[-train_idx, ]

cat("Dataset Class Distribution:\n")
cat("Overall - Default: ", round(mean(df$`default payment next month` == 1) * 100, 2), "%\n", sep="")
cat("Training Set - Default: ", round(mean(train_data$`default payment next month` == 1) * 100, 2), "%\n", sep="")
cat("Test Set - Default: ", round(mean(test_data$`default payment next month` == 1) * 100, 2), "%\n\n", sep="")

cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")
```

## Train Model on Training Set

Now we train our final model using the 18 features we selected, but only on the training data (not including any of the test set).

```{r train-model, warning=FALSE, results='hide'}
train_model <- glm(`default payment next month` ~ PAY_0 + PAY_2 + PAY_3 + PAY_5 + 
                     LIMIT_BAL + PAY_AMT1 + MARRIAGE + SEX + EDUCATION + 
                     AGE + PAY_AMT2 + PAY_AMT4 + PAY_AMT3 + PAY_AMT5 + BILL_AMT1 + 
                     PAY_AMT6 + BILL_AMT2 + BILL_AMT3,
                   data = train_data,
                   family = binomial(link = "logit"))

summary(train_model)
```

## Model Predictions

Now we use the trained model to make predictions on the test set we put aside earlier.

```{r predictions, results='hide'}
pred_prob <- predict(train_model, newdata = test_data, type = 'response')
pred_class <- ifelse(pred_prob > 0.5, 1, 0)

cat("Prediction probability range:", min(pred_prob), "to", max(pred_prob), "\n")
cat("Number of predicted defaults:", sum(pred_class == 1), "\n")
cat("Number of predicted non-defaults:", sum(pred_class == 0), "\n")
```

## Confusion Matrix and Performance Metrics

Let's see how our predictions compare to reality using a confusion matrix.

```{r confusion-matrix, results='hide'}
actual <- as.factor(as.numeric(test_data$`default payment next month`) - 1)
pred_class <- as.factor(pred_class)

cm <- confusionMatrix(pred_class, actual, positive = "1")
print(cm)
```

## Confusion Matrix Visualization

Here's the confusion matrix shown as a color chart. The darker the red, the more cases in that cell. You want to see the diagonal (top-left and bottom-right) bright red, because those are the correct predictions.

```{r confusion-matrix-heatmap, fig.width=8, fig.height=6}
# Extract confusion matrix values
cm_matrix <- cm$table

# Properly convert matrix to long format preserving the 2D structure
cm_long <- expand_grid(
  Actual = as.numeric(colnames(cm_matrix)),
  Prediction = as.numeric(rownames(cm_matrix))
) |> 
  mutate(
    Count = c(cm_matrix),
    Actual_Label = factor(Actual, labels = c("No Default (0)", "Default (1)")),
    Prediction_Label = factor(Prediction, labels = c("No Default (0)", "Default (1)"))
  )

# Create heatmap with correct orientation
ggplot(cm_long, aes(x = Actual_Label, y = Prediction_Label, fill = Count)) +
  geom_tile(color = "black", linewidth = 1) +
  geom_text(aes(label = Count), fontface = "bold", size = 6, color = "white") +
  scale_fill_gradient(low = "#fee5d9", high = "#a50f15") +
  scale_y_discrete(limits = rev(levels(cm_long$Prediction_Label))) +
  labs(title = "Confusion Matrix Heatmap",
       x = "Actual Class",
       y = "Predicted Class",
       fill = "Count",
       subtitle = "Correct predictions on diagonal (TN top-left=6815, TP bottom-right=492)") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 10, color = "gray40"),
        axis.text = element_text(size = 11),
        panel.grid = element_blank())
```

## Extract and Display Performance Metrics

Let's calculate all the important numbers that tell us how well our model did.

```{r extract-metrics}
metrics <- cm$byClass
overall_metrics <- cm$overall

accuracy <- overall_metrics["Accuracy"]
sensitivity <- metrics["Sensitivity"]
specificity <- metrics["Specificity"]
precision <- metrics["Pos Pred Value"]
balanced_acc <- metrics["Balanced Accuracy"]

# Calculate AUC for display
roc_obj <- roc(actual, pred_prob, positive = "1", direction = "<", smooth = FALSE)
auc_value <- auc(roc_obj)
```

## Our Results

```{r display-metrics}
cat("Performance Metrics with 95% Confidence Intervals:\n\n")
cat("Accuracy:              ", round(overall_metrics["Accuracy"], 4), "\n")
cat("95% CI:                [", round(overall_metrics["AccuracyLower"], 4), ", ", 
    round(overall_metrics["AccuracyUpper"], 4), "]\n\n", sep="")
cat("Sensitivity (Recall):  ", round(metrics["Sensitivity"], 4), "\n")
cat("Specificity:           ", round(metrics["Specificity"], 4), "\n")
cat("Precision:             ", round(metrics["Pos Pred Value"], 4), "\n")
cat("Balanced Accuracy:     ", round(metrics["Balanced Accuracy"], 4), "\n")
cat("AUC:                   ", round(auc_value, 4), "\n")
```


### Performance Metrics Summary

| Metric            | Value  | Interpretation                                         |
|-------------------|--------|--------------------------------------------------------|
| Accuracy          | 81.20% | Overall correct predictions across both classes        |
| Sensitivity       | 24.72% | Proportion of actual defaults correctly identified     |
| Specificity       | 97.23% | Proportion of actual non-defaults correctly identified |
| Precision         | 71.72% | Of predicted defaults, how many are actually correct   |
| Balanced Accuracy | 60.98% | Average performance accounting for class imbalance     |
| AUC               | 0.7250 | Discrimination ability between default and non-default |



```{r roc-curve, fig.width=8, fig.height=6}
# Create a clean ROC plot from scratch
plot(NA,
     main = "ROC Curve",
     xlim = c(1, 0),
     ylim = c(0, 1),
     xlab = "Specificity",
     ylab = "Sensitivity",
     xaxs = "i",
     yaxs = "i")

grid(nx = 5, ny = 5, col = "lightgray", lty = 1)

abline(1, -1, lty = 2, col = "gray", lwd = 1.5)

lines(roc_obj$specificities, roc_obj$sensitivities, col = "steelblue", lwd = 2)

legend("bottomright", paste("AUC =", round(auc_value, 4)),
            cex = 1.1, bty = "o", bg = "white")
```
