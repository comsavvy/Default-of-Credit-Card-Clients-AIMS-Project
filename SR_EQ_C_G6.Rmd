---
author: "Group 6"
date: "2025-11-24"
title: "Default of Credit Card Clients in Taiwan Dataset Analysis and Modeling using Logistic Regression in R"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

## Load Libraries

Load required packages for data manipulation, reading Excel files, and correlation visualization.

```{r load-libraries}
suppressPackageStartupMessages({
  library(tidyverse)
  library(readxl)
  library(corrplot)
  library(caret)
  library(pROC)
})
```

## Load Data

Load the credit card default dataset from Excel file, skipping the first row (header info).

```{r load-data}
df <- read_excel("datasets/default of credit card clients.xls", skip = 1)
df |> glimpse()
```

## Data Cleaning

Remove the ID column as it is not a predictive feature.

```{r clean-data}
df <- df |> select(-ID)
df |> head()
```

## Summary Statistics

Display descriptive statistics for all variables in the dataset.

```{r data-summary}
summary(df)
```

## Check Missing Values

Verify that there are no missing values in the dataset.

```{r check-missing}
colSums(is.na(df))
```

# Exploratory Data Analysis

## Target Variable Distribution

Visualize the distribution of default payment status to assess class balance.

```{r target-dist}
df |> ggplot(aes(x = factor(`default payment next month`))) +
  geom_bar(fill = "steelblue", width = 0.5) +
  labs(title = "Distribution of Default Payment",
       x = "Default (0 = No, 1 = Yes)",
       y = "Count") +
  theme_minimal()
```

## Correlation Analysis

Calculate Pearson correlations between all features and the target variable. Extract and sort the top 15 features by absolute correlation strength.

```{r correlation-matrix}
corr_matrix <- cor(df)

target_col <- "default payment next month"
target_corr <- corr_matrix[, target_col]
target_corr_sorted <- sort(abs(target_corr), decreasing = TRUE)

corr_df <- data.frame(
  Variable = names(target_corr_sorted),
  Correlation = target_corr[names(target_corr_sorted)]
) |>
  filter(Variable != target_col)

head(corr_df, 15) |>
  rownames_to_column("Index") |>
  mutate(Index = as.numeric(Index)) |>
  `rownames<-`(NULL)
```

## Correlation Bar Plot

Visualize the top 15 correlations with the target variable using a horizontal bar plot. Red bars indicate positive correlation (increases default risk), blue bars indicate negative correlation (reduces default risk).

```{r corr-barplot, fig.width=8, fig.height=5}
corr_df |>
  head(15) |>
  ggplot(aes(x = reorder(Variable, abs(Correlation)), 
             y = Correlation,
             fill = ifelse(Correlation > 0, "Positive", "Negative"))) +
  geom_bar(stat = "identity", width = 0.6) +
  coord_flip() +
  labs(title = "Top 15 Features Correlated with Default Payment",
       x = "Features",
       y = "Correlation Coefficient",
       fill = "Correlation Type") +
  scale_fill_manual(values = c("Positive" = "#3498db", "Negative" = "#e74c3c")) + #nolint
  theme_minimal() +
  theme(axis.text = element_text(size = 10),
        plot.title = element_text(size = 12, face = "bold"))
```

# Feature Selection and Modeling

## Prepare Data for Modeling

Prepare the dataset by converting the target variable to a factor for logistic regression.

```{r prepare-data}
df$`default payment next month` <- factor(df$`default payment next month`, levels = c(0, 1)) # nolint
```

## Full Logistic Regression Model

Build logistic regression model with all features.

```{r full-model, warning=FALSE, cache=TRUE}
full_model <- glm(`default payment next month` ~ .,
                  data = df,
                  family = binomial(link = "logit"))

summary(full_model)
```

## Individual Feature AIC Evaluation

Evaluate each feature's contribution to model fit using AIC.

```{r feature-aic-eval, warning=FALSE, cache=TRUE}
feature_names <- colnames(df)[colnames(df) != "default payment next month"]

feature_aic <- data.frame(
  Feature = feature_names,
  AIC = NA
)

for (i in seq_along(feature_names)) {
  formula <- paste("`default payment next month` ~", feature_names[i])
  model <- glm(as.formula(formula), data = df, family = binomial(link = "logit"))
  feature_aic$AIC[i] <- AIC(model)
}

feature_aic <- feature_aic |>
  arrange(AIC) |>
  mutate(Delta_AIC = AIC - min(AIC))

print(feature_aic)
```

PAY_0 (most recent repayment status) is the strongest individual predictor with lowest AIC of 28535.57. PAY_2 and PAY_3 follow as secondary predictors. Bill amounts and age have higher AIC values, indicating weaker individual predictive power.

## Forward Stepwise Feature Selection using AIC

We employ forward stepwise selection starting with the strongest individual predictor (PAY_0) and progressively add features that provide the greatest AIC reduction. This approach reveals the optimal feature combinations and demonstrates how model performance improves as we incorporate additional predictors.

```{r forward-selection, warning=FALSE, cache=TRUE}
# Start with the best individual feature (PAY_0)
initial_model <- glm(`default payment next month` ~ PAY_0, data = df, family = binomial(link = "logit"))

# Run forward selection from the initial model with trace to show AIC progression
step_model <- step(initial_model, scope = list(upper = full_model), direction = "forward", trace = 1)

summary(step_model)
```

## Feature Combination and AIC Progression

The table below summarizes the AIC values at each step of the forward selection process, showing how the model improves as features are added sequentially.

```{r feature-combination-table, warning=FALSE}
# Extract selected features from step_model
selected_features <- names(coef(step_model))[-1]  # Exclude intercept

# Create a table showing feature combination progression
feature_progression <- data.frame(
  Step = seq_along(selected_features),
  Feature_Added = selected_features,
  Cumulative_Features = sapply(seq_along(selected_features), function(i) 
    paste(selected_features[1:i], collapse = " + "))
)

# Calculate AIC for each step's model
aics <- numeric(length(selected_features))
aics[1] <- AIC(initial_model)

for(i in 2:length(selected_features)) {
  formula_str <- paste("`default payment next month` ~", 
                       paste(selected_features[1:i], collapse = " + "))
  temp_model <- glm(as.formula(formula_str), data = df, 
                    family = binomial(link = "logit"))
  aics[i] <- AIC(temp_model)
}

feature_progression$AIC <- aics
feature_progression$AIC_Reduction <- c(0, diff(-aics))

print(feature_progression)
```

## AIC Progression Visualization

Visualize how model fit improves as features are progressively added through forward selection.

```{r aic-progression-plot, fig.width=10, fig.height=6}
feature_progression_viz <- data.frame(
  Step = seq_along(selected_features),
  Features_Count = seq_along(selected_features),
  AIC = aics
)

ggplot(feature_progression_viz, aes(x = Features_Count, y = AIC)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(color = "steelblue", size = 3) +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.2, color = "gray") +
  labs(title = "AIC Progression Through Forward Feature Selection",
       x = "Number of Features Added",
       y = "AIC Value",
       subtitle = "Demonstrates diminishing returns as features are progressively added") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 11, color = "gray40"),
        axis.text = element_text(size = 10))
```

## Forward Selection Summary

```{r forward-comparison}
cat("Initial Model (PAY_0 only) AIC:", AIC(initial_model), "\n")
cat("Final Selected Model AIC:", AIC(step_model), "\n")
cat("Total AIC Improvement:", AIC(initial_model) - AIC(step_model), "\n")
cat("Total Features Selected:", length(coef(step_model)) - 1, "\n")
```

## Analysis of Feature Selection Results

Our forward stepwise selection process identifies 18 optimal features that collectively minimize the AIC while maintaining model interpretability. The analysis reveals several key insights:

**Initial Model Performance:** Starting with PAY_0 alone, our baseline model achieves an AIC of 28,535.57. This validates that recent repayment status is the strongest individual predictor of credit card default.

**Feature Addition Pattern:** The feature progression table demonstrates that each additional feature provides meaningful AIC reduction, particularly in the early stages: - Features 2-5 (LIMIT_BAL, PAY_3, PAY_AMT1, BILL_AMT1) yield substantial improvements of 92-199 AIC points each - Features 6-12 (MARRIAGE through SEX) provide moderate improvements of 10-39 AIC points - Features 13-18 show diminishing returns with reductions of 0.5-7.8 AIC points, but remain beneficial for model fit

**Total Improvement:** The cumulative AIC improvement from 28,535.57 to 27,917.81 represents a reduction of 617.76 points, indicating that the selected 18-feature model provides substantially better model fit than the baseline single-feature model while maintaining parsimony relative to the full 23-feature model.

**Feature Composition:** The selected features encompass multiple dimensions of creditworthiness: recent repayment status (PAY_0 through PAY_6), payment amounts (PAY_AMT1-6), bill amounts (BILL_AMT1-3), demographic characteristics (MARRIAGE, SEX, EDUCATION, AGE), and available credit (LIMIT_BAL). This diversity suggests that default prediction benefits from both behavioral and demographic information.

## Feature Importance and Coefficients

Coefficients and p-values for the selected model.

```{r feature-importance}
model_coef <- data.frame(
  Feature = names(coef(step_model)),
  Coefficient = coef(step_model)
) |>
  mutate(Coefficient = round(Coefficient, 6))

p_values <- summary(step_model)$coefficients[, "Pr(>|z|)"]
model_coef$P_Value <- p_values[match(model_coef$Feature, names(p_values))]
model_coef$Significance <- ifelse(model_coef$P_Value < 0.001, "***",
                                   ifelse(model_coef$P_Value < 0.01, "**",
                                          ifelse(model_coef$P_Value < 0.05, "*", "")))

print(model_coef)
```

## Coefficient Analysis and Interpretation

Our logistic regression model reveals that repayment status and payment behavior are the most critical determinants of default risk:

**Strongest Risk Factors (Increase Default Probability):** - **PAY_0 (coefficient: 0.579, p \< 0.001)**: The most dominant predictor. Recent payment status strongly increases default odds, with each unit increase in repayment delay status substantially raising default probability. - **PAY_2 and PAY_3 (coefficients: 0.083 and 0.082, p \< 0.001)**: Two-month and three-month repayment status similarly contribute to default risk, indicating that payment delay patterns persist and compound risk. - **PAY_5 (coefficient: 0.056, p \< 0.01)**: Five-month status also shows positive effect on default risk. - **AGE (coefficient: 0.007, p \< 0.001)**: Older customers have marginally higher default risk.

**Strongest Protective Factors (Decrease Default Probability):** - **MARRIAGE (coefficient: -0.155, p \< 0.001)**: Married individuals show substantially lower default risk compared to unmarried individuals. - **SEX (coefficient: -0.108, p \< 0.001)**: Female customers have lower default risk than male customers. - **EDUCATION (coefficient: -0.102, p \< 0.001)**: Higher education level provides protective effect against default. - **Available Credit (LIMIT_BAL, coefficient: -0.000001, p \< 0.001)**: Higher credit limits correlate with lower default risk, suggesting creditworthy customers receive higher limits. - **Payment Amounts (PAY_AMT1-6, all negative coefficients)**: Greater payment amounts strongly reduce default risk, indicating that customers making larger payments are less likely to default.

**Statistical Significance:** Most features are highly significant at p \< 0.001, demonstrating the robustness of their predictive contributions. A few features (BILL_AMT2, PAY_AMT3, PAY_AMT6) show marginal or non-significant p-values (p \> 0.05), suggesting limited unique predictive value when combined with other features, though they were selected by the forward procedure for their marginal AIC improvements.

## Feature Importance Visualization

Visualize the magnitude and direction of coefficient effects to identify the strongest risk and protective factors.

```{r coefficient-plot, fig.width=10, fig.height=8}
# Prepare data for plotting (exclude intercept)
coef_plot_data <- model_coef |>
  filter(Feature != "(Intercept)") |>
  arrange(Coefficient) |>
  mutate(Feature = factor(Feature, levels = Feature),
         Effect_Type = ifelse(Coefficient > 0, "Risk Factor", "Protective Factor"))

ggplot(coef_plot_data, aes(x = Coefficient, y = Feature, fill = Effect_Type)) +
  geom_col(width = 0.7) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black", linewidth = 0.8) +
  scale_fill_manual(values = c("Risk Factor" = "#e74c3c", "Protective Factor" = "#27ae60")) +
  labs(title = "Model Coefficients: Feature Importance",
       x = "Coefficient Value",
       y = "Feature",
       fill = "Effect Type",
       subtitle = "Red: Increases default risk | Green: Decreases default risk") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 11, color = "gray40"),
        axis.text.y = element_text(size = 9),
        axis.text.x = element_text(size = 10),
        legend.position = "bottom")
```

# Model Evaluation

## Train-Test Split

Split dataset into 70% training and 30% testing sets for model evaluation.

```{r train-test-split}
set.seed(123)
n <- nrow(df)
train_idx <- sample(1:n, size = 0.7 * n)
train_data <- df[train_idx, ]
test_data <- df[-train_idx, ]

cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")
```

## Train Model on Training Set

Fit the stepwise model on training data.

```{r train-model, warning=FALSE}
train_model <- glm(`default payment next month` ~ PAY_0 + PAY_2 + PAY_3 + PAY_5 + 
                     LIMIT_BAL + PAY_AMT1 + MARRIAGE + SEX + EDUCATION + 
                     AGE + PAY_AMT2 + PAY_AMT4 + PAY_AMT3 + PAY_AMT5 + BILL_AMT1 + 
                     PAY_AMT6 + BILL_AMT2 + BILL_AMT3,
                   data = train_data,
                   family = binomial(link = "logit"))

summary(train_model)
```

## Model Predictions

Generate predictions on test set.

```{r predictions}
pred_prob <- predict(train_model, newdata = test_data, type = "response")
pred_class <- ifelse(pred_prob > 0.5, 1, 0)

cat("Prediction probability range:", min(pred_prob), "to", max(pred_prob), "\n")
cat("Number of predicted defaults:", sum(pred_class == 1), "\n")
cat("Number of predicted non-defaults:", sum(pred_class == 0), "\n")
```

## Confusion Matrix and Performance Metrics

Calculate confusion matrix and performance metrics using caret package.

```{r confusion-matrix}
actual <- as.factor(as.numeric(test_data$`default payment next month`) - 1)
pred_class <- as.factor(pred_class)

cm <- confusionMatrix(pred_class, actual, positive = "1")
print(cm)
```

## Confusion Matrix Visualization

Visualize the confusion matrix as a heatmap for better interpretation of classification performance.

```{r confusion-matrix-heatmap, fig.width=8, fig.height=6}
# Extract confusion matrix values
cm_table <- cm$table

# Convert to data frame for ggplot
cm_data <- as.data.frame(cm_table)
colnames(cm_data) <- c("Predicted", "Actual", "Count")

# Create heatmap
ggplot(cm_data, aes(x = Actual, y = Predicted, fill = Count)) +
  geom_tile(color = "black", linewidth = 1) +
  geom_text(aes(label = Count), fontface = "bold", size = 5, color = "white") +
  scale_fill_gradient(low = "#fee5d9", high = "#a50f15") +
  labs(title = "Confusion Matrix Heatmap",
       x = "Actual Class (0 = No Default, 1 = Default)",
       y = "Predicted Class (0 = No Default, 1 = Default)",
       fill = "Count",
       subtitle = "True Negatives (top-left) and True Positives (bottom-right) indicate correct predictions") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 11, color = "gray40"),
        axis.text = element_text(size = 11),
        panel.grid = element_blank())
```

## Extract and Display Performance Metrics

Extract key metrics from the confusion matrix object.

```{r extract-metrics}
metrics <- cm$byClass
overall_metrics <- cm$overall

accuracy <- overall_metrics["Accuracy"]
sensitivity <- metrics["Sensitivity"]
specificity <- metrics["Specificity"]
precision <- metrics["Pos Pred Value"]
balanced_acc <- metrics["Balanced Accuracy"]
```

## Performance Metrics

```{r display-metrics}
cat("Performance Metrics:\n")
cat("Accuracy:         ", round(accuracy, 4), "\n")
cat("Sensitivity:      ", round(sensitivity, 4), "\n")
cat("Specificity:      ", round(specificity, 4), "\n")
cat("Precision:        ", round(precision, 4), "\n")
cat("Balanced Accuracy:", round(balanced_acc, 4), "\n")
```

## Metric Definitions

-   **Accuracy**: (TP + TN) / Total = Overall correct predictions across both classes
-   **Sensitivity (Recall)**: TP / (TP + FN) = Proportion of actual defaults correctly identified
-   **Specificity**: TN / (TN + FP) = Proportion of actual non-defaults correctly identified\
-   **Precision**: TP / (TP + FP) = Of all predicted defaults, how many are actually correct
-   **Balanced Accuracy**: (Sensitivity + Specificity) / 2 = Average of sensitivity and specificity, accounting for class imbalance

## Model Interpretation and Performance Analysis

### Performance Metrics Summary

| Metric            | Value  | Interpretation                                         |
|-------------------|--------|--------------------------------------------------------|
| Accuracy          | 81.34% | Overall correct predictions across both classes        |
| Sensitivity       | 25.09% | Proportion of actual defaults correctly identified     |
| Specificity       | 96.98% | Proportion of actual non-defaults correctly identified |
| Precision         | 69.74% | Of predicted defaults, how many are actually correct   |
| Balanced Accuracy | 61.03% | Average performance accounting for class imbalance     |

### Detailed Performance Analysis

**Accuracy (81.34%): Overall Correctness** - The model correctly classifies 81.34% of all cases (both defaults and non-defaults) - 95% confidence interval: [80.52%, 82.14%] indicates a reliable estimate

**Sensitivity (25.09%): Default Detection Rate** - Of 1,957 actual defaults in the test set, the model identifies only 491 (25.09%) - The model misses 1,466 actual defaults (false negatives) - This low sensitivity indicates the model is conservative in flagging defaults

**Specificity (96.98%): Non-Default Identification** - Of 7,043 actual non-defaults, the model correctly identifies 6,830 (96.98%) - Only 213 false positives (non-defaults incorrectly flagged as defaults) - The model excels at identifying genuine non-defaulters and avoiding unnecessary interventions

**Precision (69.74%): Prediction Reliability** - Of 704 cases predicted as defaults, 491 are actually defaults - When the model predicts a default, it is correct approximately 70% of the time - This precision level is acceptable for risk management applications

**Balanced Accuracy (61.03%): Class-Imbalance Adjusted Performance** - Average of sensitivity (25.09%) and specificity (96.98%) - Accounts for class imbalance where only 21.7% of the population are actual defaults - Demonstrates balanced performance across both classes

### Business Implications

-   **Model Conservatism**: The model rarely predicts defaults due to severe class imbalance (\~22% defaults in the dataset)
-   **High Specificity Benefit**: A 96.98% specificity minimizes false alarms to customers, avoiding unnecessary interventions
-   **Low Sensitivity Trade-off**: A 25.09% sensitivity means the model misses 3 of 4 actual defaults
-   **Use Case Fit**: This model is better suited for avoiding customer inconvenience than for catching all potential defaults
-   **Potential Improvements**: Classification threshold adjustment or class weighting techniques could improve sensitivity at the cost of specificity

## ROC Curve and AUC

Generate ROC curve and calculate AUC to evaluate model discrimination ability.

```{r roc-curve, fig.width=8, fig.height=6}
roc_obj <- roc(actual, pred_prob)
auc_value <- auc(roc_obj)

plot(roc_obj, main = "ROC Curve", col = "steelblue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright", paste("AUC =", round(auc_value, 4)))
```

### ROC Analysis and Model Discrimination

**AUC: 0.7117**

The Area Under the ROC Curve (AUC) of 0.7117 indicates good discriminative ability. The model demonstrates its effectiveness in distinguishing between default and non-default cases, performing substantially better than random classification (AUC = 0.5). While there remains room for improvement toward excellent discrimination (AUC \> 0.80), the current performance reflects a reasonable trade-off given the inherent class imbalance in credit default prediction.

The ROC curve visualizes the relationship between sensitivity (true positive rate) and specificity (1 - false positive rate) across different classification thresholds. At the standard threshold of 0.5, the model achieves the high specificity (96.98%) and low sensitivity (25.09%) observed in the confusion matrix, reflecting its conservative approach to default prediction.

## Overall Model Assessment and Conclusions

Our logistic regression model for credit card default prediction demonstrates solid performance across multiple evaluation metrics:

**Model Strengths:** - Achieves 81.34% overall accuracy on the test set - Exhibits exceptional specificity (96.98%), effectively minimizing false alarms to non-defaulting customers - Maintains reasonable precision (69.74%), ensuring that most predicted defaults warrant genuine concern - Provides interpretable coefficients that align with domain knowledge (recent payment delays increase default risk, while certain demographic factors provide protective effects)

**Model Limitations:** - Low sensitivity (25.09%) results in missing approximately 75% of actual defaults - This conservative nature reflects the severe class imbalance (approximately 78% non-defaulters, 22% defaulters) in the dataset - The AUC of 0.7117 suggests moderate discriminative power

**Practical Implications:** This model is most suitable for scenarios where avoiding false alarms (unnecessary customer interventions) is prioritized over detecting every potential default. Financial institutions using this model should consider supplementary risk assessment methods to catch high-risk accounts that the model may overlook, or employ threshold optimization to increase sensitivity for strategic accounts requiring heightened monitoring.
